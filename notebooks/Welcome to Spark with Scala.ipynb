{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Spark Logo](http://spark-mooc.github.io/web-assets/images/ta_Spark-logo-small.png)\n",
    "![Scala Logo](https://upload.wikimedia.org/wikipedia/en/8/85/Scala_logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to Apache Spark with Scala"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal with this notebook is to explore the Titanic dataset and train two classifiers that will let us determine if a certain passenger was likely to survive or not based on his/her characteristics (ex. age, gender, class). You can run this locally but one can use the same code to run in distributed fashion as we are using the RDD abstraction from Apache Spark.\n",
    "\n",
    "Some resources that we used to compose this notebook:\n",
    "\n",
    "[Spark 1.4.1 API](http://spark.apache.org/docs/1.4.1/api/scala)\n",
    "\n",
    "[Decision Trees documentation](http://spark.apache.org/docs/1.4.1/mllib-decision-tree.html)\n",
    "\n",
    "[Logistic Regression documentation](http://spark.apache.org/docs/1.4.1/mllib-linear-methods.html#logistic-regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Explore data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the [IBM Spark Kernel](https://github.com/ibm-et/spark-kernel) we already have the default spark context (ex. sc), so let's start by loading the file and inspecting our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val rawRdd = sc.textFile(\"datasets/COUNT/titanic.csv\"); println(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1317"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawRdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(\"\",\"class\",\"age\",\"sex\",\"survived\", \"1\",\"1st class\",\"adults\",\"man\",\"yes\", \"2\",\"1st class\",\"adults\",\"man\",\"yes\", \"3\",\"1st class\",\"adults\",\"man\",\"yes\", \"4\",\"1st class\",\"adults\",\"man\",\"yes\")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawRdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset as we see above contains a line with the header. Let's get rid of it and leave only the data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val header = rawRdd.first()\n",
    "val dataRdd = rawRdd.filter( _ != header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify that our new dataset no longer has the header."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"1\",\"1st class\",\"adults\",\"man\",\"yes\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataRdd.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see other data points at random. **Try replacing the last parameter ```0L``` to ```3L```, this is just a seed.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(\"1222\",\"3rd class\",\"adults\",\"women\",\"no\", \"212\",\"1st class\",\"adults\",\"women\",\"yes\", \"284\",\"1st class\",\"adults\",\"women\",\"yes\", \"249\",\"1st class\",\"adults\",\"women\",\"yes\", \"1041\",\"3rd class\",\"adults\",\"man\",\"no\")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataRdd.takeSample(false, 5, 0L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, in order to play with machine learning models we need a numerical representation of our data. Thus we need to translate our data points to [feature vectors](https://en.wikipedia.org/wiki/Feature_vector), you can think of this as just a list of numbers, where every number is a feature or an encoding of the data. Let's first process our data, keep in mind that up to here our dataRdd object has each record as a string, so we need to split that into \"columns\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val rowsRdd = dataRdd.map(line => line.split(\",\").map(_.trim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rowsRdd.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a function to convert data points to feature vectors. We need to feed ```LabeledPoint``` objects to our machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.mllib.regression.LabeledPoint\n",
    "import org.apache.spark.mllib.linalg.Vectors\n",
    "\n",
    "def toVector( row : Array[String] ) : LabeledPoint = {\n",
    "    val klass = row(1).charAt(1)-'0'.toDouble-1\n",
    "    val age = if (row(2).contains(\"adults\")) 1 else 0\n",
    "    val sex = if (row(3).contains(\"women\")) 1 else 0\n",
    "    val survived = if (row(4).contains(\"yes\")) 1 else 0\n",
    "    LabeledPoint(survived, Vectors.dense(klass,age,sex))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply our define function to every row we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val vectorsRdd = rowsRdd.map(row => toVector(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can check that our feature vectors were created correctly. Refer to our ```toVector``` function for the mapping.\n",
    "\n",
    "**Try reading. The first instance would read as: A person in \"3rd class\" who was an \"adult\" and a \"woman\" did \"not survive\" (0.0,[2.0.1.0,1.0])**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array((0.0,[2.0,1.0,1.0]), (1.0,[0.0,1.0,1.0]), (1.0,[0.0,1.0,1.0]), (1.0,[0.0,1.0,1.0]), (0.0,[2.0,1.0,0.0]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorsRdd.takeSample(false, 5, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's split the data, allocating 70% for training and 30% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val splits = vectorsRdd.randomSplit(Array(0.7, 0.3))\n",
    "val (trainingData, testData) = (splits(0), splits(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train a Decision tree model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by training a decision tree model which is popular these days, but there are cases where the structure of the data might benefit from other supervised algorithms. For our case, we specify that ```numClasses``` is 2 as we are concerned with either a **survived (1.0)** or **not survived (0.0)** prediction. Also for ```categoricalFeaturesInfo``` we specify for each feature how many outcomes we can have. Thus, for the first Map, we say that for our feature 0, we can have 3 different outcomes (ex. 1st, 2nd or 3rd class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.mllib.tree.DecisionTree\n",
    "\n",
    "val numClasses = 2\n",
    "val categoricalFeaturesInfo = Map[Int, Int]((0,3), (1,2), (2,2))\n",
    "val impurity = \"gini\"\n",
    "val maxDepth = 5\n",
    "val maxBins = 32\n",
    "\n",
    "val model = DecisionTree.trainClassifier(trainingData, numClasses, categoricalFeaturesInfo,\n",
    "                                         impurity, maxDepth, maxBins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we train, let's predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// Evaluate model on test instances and compute test error\n",
    "val labelAndPreds = testData.map { point =>\n",
    "  val prediction = model.predict(point.features)\n",
    "  (point.label, prediction)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we used only the features to predict. And below let's compute the error rate. You can see that first level if-else, is based on sex, and for many of the inner branches looks like if you were a women our model is more likely to predict 1.0 (survived)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.21119592875318066\n",
      "Learned classification tree model:\n",
      "DecisionTreeModel classifier of depth 4 with 19 nodes\n",
      "  If (feature 2 in {0.0})\n",
      "   If (feature 1 in {0.0})\n",
      "    If (feature 0 in {2.0})\n",
      "     Predict: 0.0\n",
      "    Else (feature 0 not in {2.0})\n",
      "     Predict: 1.0\n",
      "   Else (feature 1 not in {0.0})\n",
      "    If (feature 0 in {0.0})\n",
      "     Predict: 0.0\n",
      "    Else (feature 0 not in {0.0})\n",
      "     If (feature 0 in {1.0})\n",
      "      Predict: 0.0\n",
      "     Else (feature 0 not in {1.0})\n",
      "      Predict: 0.0\n",
      "  Else (feature 2 not in {0.0})\n",
      "   If (feature 0 in {0.0,1.0})\n",
      "    If (feature 0 in {0.0})\n",
      "     Predict: 1.0\n",
      "    Else (feature 0 not in {0.0})\n",
      "     If (feature 1 in {0.0})\n",
      "      Predict: 1.0\n",
      "     Else (feature 1 not in {0.0})\n",
      "      Predict: 1.0\n",
      "   Else (feature 0 not in {0.0,1.0})\n",
      "    If (feature 1 in {1.0})\n",
      "     Predict: 0.0\n",
      "    Else (feature 1 not in {1.0})\n",
      "     Predict: 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val testErr = labelAndPreds.filter(r => r._1 != r._2).count.toDouble / testData.count()\n",
    "println(\"Test Error = \" + testErr)\n",
    "println(\"Learned classification tree model:\\n\" + model.toDebugString)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Play with model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's have some fun, we can create random scenarios to see how our model would classify that data point. Remember we have to talk to our model using ```LabeledPoint``` objects. So, we need to create instances using the following:\n",
    "\n",
    "```LabeledPoint(survived, Vectors.dense(klass,age,sex))```\n",
    "\n",
    "First 3 test passengers are men in 1st, 2nd and 3rd class. The last one is a girl. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val testPassenger1 = LabeledPoint(0.0, Vectors.dense(0.0,1,0,0.0))\n",
    "val testPassenger2 = LabeledPoint(0.0, Vectors.dense(1.0,1,0,0.0))\n",
    "val testPassenger3 = LabeledPoint(0.0, Vectors.dense(2.0,1,0,0.0))\n",
    "val testPassenger4 = LabeledPoint(1.0, Vectors.dense(0.0,0,0,1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "println(model.predict(testPassenger4.features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Seems that if you were a **girl in 1st class** our model says **you were likely to survive.** On the other hand, if you were a man regardless of the class your chances were not that good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train a linear regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train another model using our same ```trainingData``` object we created at the end of Section 2 for comparison purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.mllib.regression.LinearRegressionWithSGD\n",
    "\n",
    "val numIterations = 100\n",
    "val linearRegressionModel = LinearRegressionWithSGD.train(trainingData, numIterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// Compute raw scores on the test set.\n",
    "val scoreAndLabels = testData.map { point =>\n",
    "  val score = linearRegressionModel.predict(point.features)\n",
    "  (score, point.label)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under ROC = 0.7938241963452048\n"
     ]
    }
   ],
   "source": [
    "// Get evaluation metrics.\n",
    "import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics\n",
    "\n",
    "val metrics = new BinaryClassificationMetrics(scoreAndLabels)\n",
    "val auROC = metrics.areaUnderROC()\n",
    "\n",
    "println(\"Area under ROC = \" + auROC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice we got ROC value of 79%, [ROC](http://stats.stackexchange.com/questions/18178/measuring-accuracy-of-a-logistic-regression-based-model) is just another way to calculate precision of the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.10.4",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "name": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
